# -*- coding: utf-8 -*-
"""
Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ è‡ªå‹•åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆJMSï¼Mobility Show å¯¾å¿œãƒ»1æ™‚é–“ã”ã¨å®Ÿè¡Œï¼‰

æ©Ÿèƒ½æ¦‚è¦ï¼š
- æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼šã€ŒJMSã€ã€Œãƒ¢ãƒ“ãƒªãƒ†ã‚£ã‚·ãƒ§ãƒ¼ã€ã€Œmobility showã€
- å‡ºåŠ›å…ˆã‚·ãƒ¼ãƒˆï¼šã€ŒYahooã€ï¼ˆ1æšã§å…¨å±¥æ­´ã‚’ç®¡ç†ï¼‰
- URLé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—
- ã‚³ãƒ¡ãƒ³ãƒˆæœ€å¤§5000ä»¶ã€1ã‚»ãƒ«ã‚ãŸã‚Š50ä»¶(JSONé…åˆ—)ã§åˆ†å‰²è¨˜éŒ²ï¼ˆOã€œZåˆ—ï¼‰
- æŠ•ç¨¿æ—¥ã¨å–å¾—æ—¥æ™‚ã¯æ—¥æœ¬æ™‚é–“ï¼ˆJSTï¼‰
"""

import os
import json
import time
import re
from datetime import datetime, timedelta, timezone
from typing import List, Tuple

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# ====== è¨­å®š ======
SPREADSHEET_ID = os.environ.get("SPREADSHEET_ID", "1-oJl5lnTC2FRqayHsPW1ZVqfktAE99PCK6HGmKHiR28")
SHEET_NAME = "Yahoo"
KEYWORDS = ["JMS", "ãƒ¢ãƒ“ãƒªãƒ†ã‚£ã‚·ãƒ§ãƒ¼", "mobility show"]

MAX_BODY_PAGES = 10
MAX_TOTAL_COMMENTS = 5000
COMMENTS_PER_CELL = 50  # ã‚³ãƒ¡ãƒ³ãƒˆ1ã‚»ãƒ«ã‚ãŸã‚Šæœ€å¤§ä»¶æ•°
REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))


# ====== å…±é€šé–¢æ•° ======
def jst_now() -> datetime:
    return datetime.now(TZ_JST)


def format_datetime(dt_obj) -> str:
    return dt_obj.strftime("%Y/%m/%d %H:%M")


def to_jst_from_str(raw: str) -> str:
    """Yahooä¸Šã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’JSTå½¢å¼ 'YYYY/MM/DD HH:MM' ã«çµ±ä¸€"""
    if not raw:
        return "å–å¾—ä¸å¯"
    raw = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', raw).strip()
    for fmt in ("%Y/%m/%d %H:%M", "%m/%d %H:%M"):
        try:
            dt = datetime.strptime(raw, fmt)
            if fmt == "%m/%d %H:%M":
                dt = dt.replace(year=jst_now().year)
            return format_datetime(dt)
        except ValueError:
            continue
    return raw


def chunk(lst: List[str], size: int) -> List[List[str]]:
    return [lst[i:i + size] for i in range(0, len(lst), size)]


# ====== Googleèªè¨¼ ======
def build_gspread_client() -> gspread.Client:
    creds_str = os.environ.get("GOOGLE_CREDENTIALS")
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    if creds_str:
        info = json.loads(creds_str)
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
        return gspread.authorize(credentials)
    else:
        with open('credentials.json', 'r', encoding='utf-8') as f:
            credentials = json.load(f)
        return gspread.service_account_from_dict(credentials)


# ====== Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ ======
def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    print(f"ğŸ” æ¤œç´¢ä¸­: {keyword}")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1280,1024")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    results = []
    for a in articles:
        try:
            title_tag = a.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            link_tag = a.find("a", href=True)
            url = link_tag["href"] if link_tag else ""
            time_tag = a.find("time")
            date_str = to_jst_from_str(time_tag.text.strip() if time_tag else "å–å¾—ä¸å¯")
            source_tag = a.find("div", class_="sc-n3vj8g-0 yoLqH")
            site = source_tag.text.strip() if source_tag else "å–å¾—ä¸å¯"

            if title and url:
                results.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": date_str, "æ²è¼‰å…ƒ": site})
        except Exception:
            continue
    print(f"âœ… {len(results)}ä»¶å–å¾— ({keyword})")
    return results


# ====== æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ======
def fetch_article_pages(base_url: str) -> List[str]:
    bodies = []
    for page in range(1, MAX_BODY_PAGES + 1):
        url = base_url if page == 1 else f"{base_url}?page={page}"
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=10)
            res.raise_for_status()
        except Exception:
            break
        soup = BeautifulSoup(res.text, "html.parser")
        article = soup.find("article") or soup.find("main")
        if not article:
            break
        ps = article.find_all("p")
        text = "\n".join(p.get_text(strip=True) for p in ps if p.get_text(strip=True))
        if not text or (bodies and text == bodies[-1]):
            break
        bodies.append(text)
    return bodies


def fetch_comments(base_url: str) -> List[List[str]]:
    """ã‚³ãƒ¡ãƒ³ãƒˆæœ€å¤§5000ä»¶ã‚’å–å¾—ã—ã€50ä»¶å˜ä½ã§åˆ†å‰²"""
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1280,2000")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    comments = []
    page = 1
    try:
        while len(comments) < MAX_TOTAL_COMMENTS:
            c_url = f"{base_url}/comments?page={page}"
            driver.get(c_url)
            time.sleep(2)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            elems = soup.select("p.sc-169yn8p-10, div.commentBody, p[data-ylk*='cm_body']")
            page_comments = [e.get_text(strip=True) for e in elems if e.get_text(strip=True)]
            if not page_comments:
                break
            comments.extend(page_comments)
            if len(page_comments) < 10:  # æœ€çµ‚ãƒšãƒ¼ã‚¸åˆ¤å®š
                break
            page += 1
    finally:
        driver.quit()

    # æœ€å¤§5000ä»¶ã«åˆ¶é™ã—ã€50ä»¶å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
    comments = comments[:MAX_TOTAL_COMMENTS]
    comment_cells = chunk(comments, COMMENTS_PER_CELL)
    return comment_cells


# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ ======
def ensure_yahoo_sheet(gc: gspread.Client):
    sh = gc.open_by_key(SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=SHEET_NAME, rows="1000", cols="100")
        ws.append_row(
            ["ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "æ²è¼‰å…ƒ", "å–å¾—æ—¥æ™‚"]
            + [f"æœ¬æ–‡({i}ãƒšãƒ¼ã‚¸)" for i in range(1, MAX_BODY_PAGES + 1)]
            + ["ã‚³ãƒ¡ãƒ³ãƒˆæ•°"]
            + [f"ã‚³ãƒ¡ãƒ³ãƒˆ({i*50-49}ã€œ{i*50})" for i in range(1, 101)]
        )
    return ws


def append_to_sheet(ws, data: List[List[str]]):
    if data:
        ws.append_rows(data, value_input_option="USER_ENTERED")
        print(f"ğŸ“ {len(data)}è¡Œè¿½åŠ ã—ã¾ã—ãŸã€‚")


# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† ======
def main():
    gc = build_gspread_client()
    ws = ensure_yahoo_sheet(gc)
    existing_urls = set(ws.col_values(3)[1:])  # URLåˆ—(C)

    all_articles = []
    for kw in KEYWORDS:
        all_articles.extend(get_yahoo_news_with_selenium(kw))
        time.sleep(1)

    new_rows = []
    for art in all_articles:
        url = art["URL"]
        if url in existing_urls:
            continue

        title = art["ã‚¿ã‚¤ãƒˆãƒ«"]
        date = art["æŠ•ç¨¿æ—¥"]
        site = art["æ²è¼‰å…ƒ"]
        timestamp = format_datetime(jst_now())

        bodies = fetch_article_pages(url)
        comment_cells = fetch_comments(url)
        comment_jsons = [json.dumps(pg, ensure_ascii=False) for pg in comment_cells]
        comment_count = sum(len(pg) for pg in comment_cells)

        row = (
            ["Yahoo", title, url, date, site, timestamp]
            + bodies[:MAX_BODY_PAGES] + [""] * (MAX_BODY_PAGES - len(bodies))
            + [comment_count] + comment_jsons
        )
        new_rows.append(row)

    if new_rows:
        append_to_sheet(ws, new_rows)
    else:
        print("âš ï¸ æ–°è¦è¨˜äº‹ãªã—ã€‚")


if __name__ == "__main__":
    main()
